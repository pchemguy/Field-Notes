# Resurrection of the Large File Downloading Issue

In the early days of the Internet with slow unreliable connections, the facilitation of file downloading task rightfully attracted considerable attention. However, as the available network bandwidth has rapidly increased and protocol resilience improved, the downloading process has progressively become more robust. Surprisingly, no major browser provider (at least on Windows), be it Microsoft, Mozilla, or Google has ever cared to develop a robust built-in download manager. The problem was only addressed by third-party solutions, and their gradual abandonment (see, e.g., the "General Information" table in [this Wikipedia article](https://en.wikipedia.org/wiki/Comparison_of_download_managers)) signaled the improved downloading experience.

Meanwhile, distribution of rapidly swelling packages, such as operating systems and CAD systems, has been also increasingly shifted towards downloading over the Internet. The fact that modern browsers are absolutely not capable of properly handling large file downloads is indirectly and silently acknowledged by many major software vendors. Instead of providing a proper robust general-purpose solution, Microsoft, Google, Autodesk, and various other vendors increasingly adopted a ridiculous solution where instead of providing a direct download link to the target distribution, provided link downloads a stub, which is essentially a download manager focusing on one specific package or a group of packages. While this design with specialized managers is justified under certain scenarios, such as in the case of highly customizable multi-component downloads, most of the time I interpret this design as a sign of gross incompetence. For example, for years, Google provided the Chrome browser as a stubbed download and implemented a separate specialized solution for downloading updates instead of integrating a proper download manager. (Another vivid example is GitHub Desktop, which has a horrible built-in download manager.) These poor-man approaches to the downloading task partially masked resurrection of the issue.

The downloading problem has been further worsened due to two tendencies. On the one hand, the size of large files has grown substantially faster than generally available network bandwidth for end users and download providers (especially those not being able to use CDN networks). On the other hand, providers have largely moved from providing direct download links to providing "nominal" download links that must be used by the browser (or download manager) to obtain real, often dynamic, direct links in the course of "download negotiation" process. This negotiation process is often used by providers to automatically balance downloads from multiple sites and/or protect the resource from automatic mass downloads (e.g., by requiring a mandatory user involvement in the download negotiation process in the form of a captcha challenge or including a short-lived dynamically generated token in the final negotiated download link (e.g., GitHub assets downloads)). Perhaps, the only reliable general-purpose solution to large file downloads is the peer-to-peer Torrent protocol, but very few providers, if any, bother providing downloads over the Torrent protocol.

Resumed downloads is the primary means that is absolutely essential for robust downloading of large files over unreliable connections (regardless of bottleneck location on the client side, server side, or somewhere in the middle). However, in cases where Recently, I have increasingly experienced problems with 