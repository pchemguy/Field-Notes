# Resurrection of the Large File Downloading Issue

In the early days of the Internet with slow unreliable connections, the facilitation of file downloading task rightfully attracted considerable attention. However, as the available network bandwidth has rapidly increased and protocol resilience improved, the downloading process has progressively become more robust. Surprisingly, no major browser provider (at least on Windows), be it Microsoft, Mozilla, or Google has ever cared to develop a robust built-in download manager. The problem was only addressed by third-party solutions, and their gradual abandonment (see, e.g., the "General Information" table in [this Wikipedia article](https://en.wikipedia.org/wiki/Comparison_of_download_managers)) signaled the improved downloading experience.

Meanwhile, distribution of rapidly swelling packages, such as operating systems and CAD systems, has been also increasingly shifted towards downloading over the Internet. The fact that modern browsers are absolutely not capable of properly handling large file downloads is indirectly and silently acknowledged by many major software vendors. Instead of providing a proper robust general-purpose solution, Microsoft, Google, Autodesk, and various other vendors increasingly adopted a ridiculous solution where instead of providing a direct download link to the target distribution, provided link downloads a stub, which is essentially a download manager focusing on one specific package or a group of packages. While this design with specialized managers is justified under certain scenarios, such as in the case of highly customizable multi-component downloads, most of the time I interpret this design as a sign of gross incompetence. For example, for years, Google provided the Chrome browser as a stubbed download and implemented a separate specialized solution for downloading updates instead of integrating a proper download manager. (Another vivid example is GitHub Desktop, which has a horrible built-in download manager.) These poor-man approaches to the downloading task partially masked resurrection of the issue.

The downloading problem has been further worsened due to two tendencies. On the one hand, the size of large files has grown substantially faster than generally available network bandwidth for end users and download providers (especially those not being able to use CDN networks). On the other hand, providers have largely moved from providing direct download links to providing "nominal" download links that must be used by the browser (or download manager) to obtain real, often dynamic, direct links in the course of "download negotiation" process. This negotiation process is often used by providers to automatically balance downloads from multiple sites and/or protect the resource from automatic mass downloads (e.g., by requiring a mandatory user involvement in the download negotiation process in the form of a captcha challenge or including a short-lived dynamically generated token in the final negotiated download link (e.g., GitHub assets downloads)). Perhaps, the only reliable general-purpose solution to large file downloads is the peer-to-peer Torrent protocol, but very few providers, if any, bother providing downloads over the Torrent protocol.

Resumed downloads is the primary means that is absolutely essential for robust downloading of large files over unreliable connections (regardless of bottleneck location on the client side, server side, or somewhere in the middle). However, in cases of dynamically negotiated links when connection error is not a temporary connectivity issue, but a result of an expired link, attempts to resume the download using the same expired link are useless. Browsers may not be capable of recognizing this matter and properly renegotiating a new link automatically even when such capability is available. In some cases, a new link (or new cookie) must be obtained by passing a new captcha challenge, so generally, cannot be performed automatically without AI. In either case, browsers fail download and delete partial files instead of letting user intervene and resume download. In such cases, it may be virtually impossible to download the file not because of any abusive user actions, but because of junk built-in download managers.

The only workable solution I have found is via scripted download using an established command-line download tool, such as WGET. While I have no intention to circumvent anti-abusive features and I am not aiming for any kind of automated abusive mass download (I simply need to be able to download the target at all), special measures needs to be taken to ensure that WGET download request will not be rejected. Servers often use identification information provided by clients in HTTP headers (mainly, user agent and cookies) to distinguish browser-initiated versus non-browser downloads to reject the latter due to there frequent use for automated mass downloads. Therefore, WGET needs to identify itself in the download requests as a browser. This task can be accomplished by retrieving headers from HTTP request sent by the browser via built-in developer tools (while this task is relatively quick, I am intentionally on providing details on this part; besides, this information can be found elsewhere) and including them in WGET command line. HTTP request metadata also provide the actual negotiated URL and cookie, which may be necessary, especially when captcha challenge is performed.