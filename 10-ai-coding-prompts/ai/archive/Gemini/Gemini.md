https://gemini.google.com/u/1/app/869d0335526f4933

# Meta AI Coding / Meta-Prompting for Coding

You are to act as a highly qualified prompt engineering (PE) expert and a peer PE developer. My/our objective is development of a suite of prompts (meta-prompts, context setting prompts, AI agent prompts, etc.) for AI-assisted (both interactive and agentic) software development.

## A

Hello! It is a pleasure to collaborate with you. I am ready to act as your peer in prompt engineering.

To get started, could you clarify the primary focus or scope of the suite we are developing? For example:

* **Target LLMs:** Are we optimizing for specific models (e.g., GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) or aiming for model-agnostic prompts?
* **Development Phase:** Are we focusing on architectural design, code generation, debugging, testing, or the entire SDLC?
* **Interaction Style:** Are you looking for conversational prompts (interactive pair programming) or autonomous agent instructions (task hand-off)?

Once we establish these parameters, we can begin drafting the meta-prompts.

In the meantime, based on standard meta-prompting frameworks, I suggest we structure our suite around the following core components.

### Suggested Framework for the Prompt Suite

#### 1. The "Architect" Meta-Prompt

**Purpose:** To establish high-level context, constraints, and design patterns before any code is written. This prompt sets the "rules of engagement" for the AI.
**Key Elements:**

* Role definition (e.g., Senior Solutions Architect).
* Technology stack constraints (e.g., "Python 3.11+, Pydantic v2, FastAPI").
* Design principles (e.g., DRY, SOLID, functional vs. OOP preference).
* Output format (e.g., "Always provide a file structure tree before coding").

#### 2. The "Code Generator" Prompt

**Purpose:** The workhorse prompt for generating specific modules or functions.
**Key Elements:**

* Input acceptance (accepting snippets from the Architect).
* Step-by-step reasoning (Chain of Thought) requirement ("First explain the logic, then write the code").
* Error handling and typing strictness standards.

#### 3. The "QA / Reviewer" Agent Prompt

**Purpose:** To critique code generated by other instances or the user.
**Key Elements:**

* Security audit instructions (OWASP Top 10).
* Performance optimization checks.
* " Devil's Advocate" mode to find edge cases.

#### 4. The "Documentation" Prompt

**Purpose:** To generate docstrings, READMEs, and technical specs.
**Key Elements:**

* Style guide adherence (e.g., Google Style, NumPy style).
* Mermaid diagram generation for visual documentation.

---

## B


An excellent clarification. Targeting SOTA reasoning models (Gemini Pro/Ultra, o1/GPT-4o) allows us to leverage their high-context windows and advanced "Chain of Thought" capabilities. The distinction between **Exploratory Interactive Mode** (for the messy, creative front-end of development) and **Formalized Agentic Mode** (for execution and rigor) is the perfect architectural split.

Here is a proposed structure for our **Meta-Prompt Suite**, broken down by the modes you identified.

### **Phase 1: The Foundation (Context Setting)**

Before we split into Interactive vs. Agentic, we need a "System Prime" prompt. This prompt loads the necessary cognitive frameworks into the LLM's context.

#### **Meta-Prompt 1: The "Cognitive Prime" (System Prompt)**

* **Purpose:** To force the LLM out of "chatbot" mode and into "Senior Engineer" mode. It establishes the reasoning protocol.
* **Target Models:** Gemini 1.5 Pro, GPT-4o.
* **Core Instruction:** "You are an expert software architect and developer. You do not just write code; you reason about system design, trade-offs, and maintainability. You strictly adhere to the following interaction protocol: [Protocol Definition]."

---

### **Phase 2: Interactive Pair Programming (Exploratory)**

This mode focuses on "Idea → Prototype." The goal is low friction, high creativity, and rapid iteration. The prompt must encourage the LLM to ask clarifying questions rather than making assumptions.

#### **Meta-Prompt 2: The "Socratic Co-Pilot"**

* **Use Case:** You have a vague idea (e.g., "I want a Python app to track my mood based on Spotify listening history") and need to flesh it out.
* **Mechanism:**
    1. **Requirement Extraction:** The LLM must interview you to define scope.
    2. **Breadth-First Search:** It generates 2-3 high-level architectural options (e.g., "Option A: CLI tool," "Option B: Web App with FastAPI," "Option C: Local GUI with Tkinter") before writing a single line of code.
    3. **Iterative Coding:** Writes code in small, testable chunks, pausing for user feedback.

**Drafting Strategy for this Prompt:**

> *Constraint:* "Do not output the full codebase at once. Output the directory structure first. Then, ask the user which module they want to implement first."

---

### **Phase 3: Agentic Task Hand-off (Formalized)**

This mode focuses on "Spec → Execution." The goal is high reliability, strict adherence to standards, and minimal human intervention.

#### **Meta-Prompt 3: The "Autonomous Implementer"**

* **Use Case:** You have a defined task (e.g., "Write a Pydantic v2 model for this JSON schema and generate a PyTest suite for it").
* **Mechanism:**
    1. **Input Validation:** Checks if the task description is sufficient. If not, it fails fast and requests specifics.
    2. **Plan-Execute-Refine:**
        * **Plan:** Writes a pseudocode or step-by-step plan in natural language.
        * **Execute:** Writes the code in a single artifact.
        * **Self-Correction:** (Crucial for reasoning models) specifically instructed to "Review the code against the requirements and fix potential bugs *before* outputting the final block."
    3. **Output Standardization:** strict formatting (e.g., "File: `main.py`", "File: `tests/test_main.py`").

**Drafting Strategy for this Prompt:**

> *Technique:* "Pseudo-code First." Force the model to write the logic in comments or pseudocode *inside* the response before generating the actual code block. This utilizes the "thinking" time of the model to reduce logic errors.

---

### **Phase 4: The Meta-Prompt Generator**

To make this sustainable, we can create a meta-prompt that generates *new* prompts for specific tasks.

#### **Meta-Prompt 4: The "Prompt Engineer"**

* **Input:** "I need a prompt for a React Component generator agent."
* **Output:** A highly structured system prompt tailored for that specific tech stack and task, incorporating the "Cognitive Prime" principles.

---

### **Next Step: Collaborative Drafting**

Which of these would you like to build first?

1. **The "Cognitive Prime" System Prompt:** The base layer for all interactions.
2. **The "Socratic Co-Pilot" (Interactive):** To start iterating on a new idea you might have.
3. **The "Autonomous Implementer" (Agentic):** If you want to test task delegation immediately.

(My recommendation: We start with the **Cognitive Prime**, as it defines the "personality" and standards for both the interactive and agentic modes.)
